{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "## Introduction\n",
    "\n",
    "**Datalizer** is a Python package designed to simplify early-stage data analysis.\n",
    "\n",
    "Its goal is to help users:\n",
    "- Load and validate structured datasets\n",
    "- Detect and resolve common data issues (e.g. missing values, duplicates)\n",
    "- Prepare numerical data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Datalizer Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalizer as dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data\n",
    "\n",
    "The first step in any data project is to load your dataset.  \n",
    "`Datalizer` provides a convenient function called `load_data()` that reads `.csv`, `.xlsx`, or `.json` files and ensures that the dataset is entirely numerical.\n",
    "\n",
    "If non-numeric data is detected, `load_data()` will raise an error — helping you catch issues early in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>68</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>75</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>80</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>80</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  weight  height\n",
       "0   25      68   175.0\n",
       "1   30      75   180.0\n",
       "2   22      60   165.0\n",
       "3   35      80   185.0\n",
       "4   35      80   185.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify a file path\n",
    "file_path = \"sample_numerical.csv\"\n",
    "\n",
    "# Load a sample dataset\n",
    "df = dl.load_data(file_path=file_path)\n",
    "\n",
    "# Show the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Issues\n",
    "\n",
    "Before cleaning a dataset, it's good practice to identify any existing problems.  \n",
    "The `check_for_issues()` function quickly inspects your dataset and reports:\n",
    "\n",
    "- Number of missing values\n",
    "- Number of duplicate rows\n",
    "- Displays the problematic rows, if there are any\n",
    "\n",
    "This step helps you decide what kind of cleaning strategy to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing cells: 1\n",
      "\n",
      "Rows with missing values:\n",
      "   age  weight  height\n",
      "5   28      70     NaN\n",
      "\n",
      "Number of duplicate rows: 1\n",
      "\n",
      "Duplicate rows:\n",
      "   age  weight  height\n",
      "4   35      80   185.0\n"
     ]
    }
   ],
   "source": [
    "dl.check_for_issues(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset\n",
    "\n",
    "Once issues are detected, you can clean the dataset using `clean_basic()`.\n",
    "\n",
    "This function:\n",
    "- Removes duplicate rows\n",
    "- Handles missing values based on a selected strategy:\n",
    "  - `\"mean\"` – fill missing values with column means\n",
    "  - `\"median\"` – fill with column medians\n",
    "  - `\"mode\"` – fill with most frequent values\n",
    "  - `\"drop\"` – remove rows with missing values entirely\n",
    "\n",
    "By default, `clean_basic()` returns a new cleaned DataFrame without modifying the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values detected. Cleaning with strategy: 'drop'.\n",
      "\n",
      "Data after cleaning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>68</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>75</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>80</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  weight  height\n",
       "0   25      68   175.0\n",
       "1   30      75   180.0\n",
       "2   22      60   165.0\n",
       "3   35      80   185.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dl.clean_basic(df, strategy=\"drop\")\n",
    "\n",
    "print(\"\\nData after cleaning:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "The dataset used in this notebook is sourced from **\"Within-Project Defect Prediction for Ansible\"** by **Elif Ceren Gok**. It is available on **OpenML** at the following link: [OpenML Dataset](https://www.openml.org/search?type=data&status=active&id=43357).\n",
    "\n",
    "### Data Preprocessing\n",
    "Using `preprocess_data()`, we perform the following steps:\n",
    "\n",
    "- **Merging** the feature set (`X_train`) and target dataset (`y_train`) on the `id` column.\n",
    "- **Removing correlated features**: Highly correlated features are identified and removed from the dataset to reduce multicollinearity.\n",
    "- **Splitting the data**: The dataset is split into training and validation sets for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped correlated features:\n",
      " ['additions_max', 'code_churn_max', 'num_tasks', 'delta_num_keys', 'delta_num_tasks', 'delta_num_tokens']\n"
     ]
    }
   ],
   "source": [
    "# Acknowledgement\n",
    "# The following dataset is sourced from \"Within-Project Defect Prediction for Ansible\" by user Elif Ceren Gok. https://www.openml.org/search?type=data&status=active&id=43357\n",
    "\n",
    "# File paths for the dataset\n",
    "X_file_path = \"X_train.csv\"\n",
    "y_file_path = \"y_train.csv\"\n",
    "\n",
    "# Load the data using the datalizer loader function\n",
    "X = dl.load_data(file_path=X_file_path)\n",
    "y = dl.load_data(file_path=y_file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "# Merge the datasets on 'id', split into training and validation sets, remove correlated features, and return the result\n",
    "X_train_split, X_val_split, y_train_split, y_val_split, dropped_corr_feats = dl.preprocess_data(X, y, merge_col=\"id\", val=True, target_col=\"failure_prone\", remove_corr=True)\n",
    "\n",
    "# Output the dropped features due to high correlation\n",
    "print(\"Dropped correlated features:\\n\",dropped_corr_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Recommendations\n",
    "\n",
    "After preprocessing the data, we can use the `recommend_approach` function to analyze the dataset and get recommendations for:\n",
    "\n",
    "1. **Task identification**: Determining if this is a classification or regression problem\n",
    "2. **Data characteristics analysis**: Examining dataset size, class balance, and feature dimensions\n",
    "3. **Model recommendations**: Suggesting appropriate models based on the dataset characteristics\n",
    "4. **Overfitting prevention**: Recommending strategies to prevent overfitting\n",
    "5. **Evaluation metrics**: Suggesting appropriate metrics for model evaluation\n",
    "\n",
    "The function performs a comprehensive analysis of the preprocessed data and provides tailored recommendations for modeling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Analysis:\n",
      "Task type: binary classification\n",
      "Dataset size: Large dataset\n",
      "Class balance: Severe imbalance detected\n",
      "Class ratio: 0.0651\n",
      "\n",
      "Top 3 Recommended Models:\n",
      "1. Logistic Regression\n",
      "   Strengths: Good interpretability, Works well with linear decision boundaries, Fast to train, Provides probabilities\n",
      "   Key hyperparameters: C (regularization strength), penalty type (L1/L2)\n",
      "\n",
      "2. Random Forest Classifier\n",
      "   Strengths: Handles non-linear relationships well, Good with high-dimensional data, Robust to outliers, Provides feature importance\n",
      "   Key hyperparameters: n_estimators, max_depth, min_samples_leaf, max_features\n",
      "\n",
      "3. Gradient Boosting Classifier\n",
      "   Strengths: Often achieves state-of-the-art performance, Handles non-linear relationships well, Good with imbalanced data\n",
      "   Key hyperparameters: learning_rate, n_estimators, max_depth, subsample\n",
      "\n",
      "Recommended Strategies to Prevent Overfitting:\n",
      "\n",
      "General strategies:\n",
      "- Use cross-validation to estimate model performance\n",
      "- Monitor training vs. validation performance\n",
      "- Start with simpler models and increase complexity as needed\n",
      "\n",
      "Regularization strategies:\n",
      "- For linear models: Add L1 or L2 regularization (Ridge or Lasso)\n",
      "- For tree models: Limit tree depth, increase min_samples_leaf\n",
      "\n",
      "Data strategies:\n",
      "- Feature selection to reduce dimensionality\n",
      "- More data collection if possible\n",
      "- Data augmentation techniques (where applicable)\n",
      "\n",
      "Suggested Evaluation Metrics:\n",
      "- Precision\n",
      "- Recall\n",
      "- F1-score\n",
      "- Precision-Recall AUC\n",
      "- Balanced accuracy\n",
      "- Cohen's Kappa\n"
     ]
    }
   ],
   "source": [
    "# Model Selection Recommendations\n",
    "# ------------------------------\n",
    "# Using the preprocessed data to get model recommendations\n",
    "\n",
    "# Get recommendations based on dataset characteristics\n",
    "recommendations = dl.recommend_approach(X_train_split, y_train_split)\n",
    "\n",
    "# Extract and display key information\n",
    "print(\"Dataset Analysis:\")\n",
    "print(f\"Task type: {recommendations['task']}\")\n",
    "print(f\"Dataset size: {recommendations['dataset_size']['status']}\")\n",
    "\n",
    "# Display class balance information if it's a classification task\n",
    "if 'classification' in recommendations['task']:\n",
    "    print(f\"Class balance: {recommendations['imbalance']['status']}\")\n",
    "    print(f\"Class ratio: {recommendations['imbalance']['ratio']:.4f}\")\n",
    "\n",
    "# Display top recommended models\n",
    "print(\"\\nTop 3 Recommended Models:\")\n",
    "for i, model in enumerate(recommendations['recommended_models'][:3], 1):\n",
    "    print(f\"{i}. {model['name']}\")\n",
    "    print(f\"   Strengths: {', '.join(model['strengths'])}\")\n",
    "    print(f\"   Key hyperparameters: {', '.join(model['hyperparameters'])}\")\n",
    "    print()\n",
    "\n",
    "# Display overfitting prevention strategies\n",
    "print(\"Recommended Strategies to Prevent Overfitting:\")\n",
    "for strategy_type, strategies in recommendations['overfitting_strategies'].items():\n",
    "    print(f\"\\n{strategy_type.capitalize()} strategies:\")\n",
    "    for strategy in strategies:\n",
    "        print(f\"- {strategy}\")\n",
    "\n",
    "# Display suggested evaluation metrics\n",
    "print(\"\\nSuggested Evaluation Metrics:\")\n",
    "for metric in recommendations['suggested_metrics']:\n",
    "    print(f\"- {metric}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
